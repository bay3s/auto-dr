from typing import Tuple, Any, List
import numpy as np

import gym
from gym.utils import EzPickle
from gym import spaces

from auto_dr.envs.base_randomized_env import BaseRandomizedEnv
from auto_dr.randomization.randomization_parameter import RandomizationParameter
from auto_dr.randomization.randomization_bound_type import RandomizationBoundType
from auto_dr.randomization.randomization_bound import RandomizationBound


class NavigationEnv(EzPickle, BaseRandomizedEnv):
    RANDOMIZABLE_PARAMETERS = [
        RandomizationParameter(
            name="x_position",
            lower_bound=RandomizationBound(
                type=RandomizationBoundType.LOWER_BOUND,
                value=0.0,
                min_value=-1.0,
                max_value=0.0,
            ),
            upper_bound=RandomizationBound(
                type=RandomizationBoundType.UPPER_BOUND,
                value=0.0,
                min_value=0.0,
                max_value=1.0,
            ),
            delta=0.03,
        ),
        RandomizationParameter(
            name="y_position",
            lower_bound=RandomizationBound(
                type=RandomizationBoundType.LOWER_BOUND,
                value=0.0,
                min_value=-1.0,
                max_value=0.0,
            ),
            upper_bound=RandomizationBound(
                type=RandomizationBoundType.UPPER_BOUND,
                value=0.0,
                min_value=0.0,
                max_value=1.0,
            ),
            delta=0.03,
        ),
    ]

    def __init__(
        self,
        max_episode_steps: int = 100,
        randomizable_parameters: List[RandomizationParameter] = RANDOMIZABLE_PARAMETERS,
        seed: int = None,
        domain_randomized: bool = False,
    ):
        """
        2D navigation problems, as described in [1]. The code is adapted from https://github.com/cbfinn/maml_rl/

        At each time step, the 2D agent takes an action (its velocity, clipped in [-0.1, 0.1]), and receives a penalty
        equal to its L2 distance to the goal position (i.e. the reward is `-distance`).

        The 2D navigation tasks are generated by sampling goal positions from the uniform distribution on [-0.5, 0.5]^2.

        [1] Chelsea Finn, Pieter Abbeel, Sergey Levine, "Model-Agnostic Meta-Learning for Fast Adaptation of Deep
        Networks", 2017 (https://arxiv.org/abs/1703.03400)

        Args:
            max_episode_steps (int): Max number of episode steps.
            randomizable_parameters (List[RandomizationParameter]): List of randomized parameters and specs.
            seed (int): Random seed for sampling.
            domain_randomized (bool): Whether this is domain randomized.
        """
        EzPickle.__init__(self)
        BaseRandomizedEnv.__init__(self, seed)

        self.viewer = None
        self._max_episode_steps = max_episode_steps
        self._elapsed_steps = 0

        self._num_dimensions = 2
        self._start_state = np.zeros(self._num_dimensions, dtype=np.float32)

        # params
        self._randomized_parameters = self._init_params(randomizable_parameters)
        self._domain_randomized = domain_randomized

        # sampled
        self._current_state = None
        self._goal_position = None

        # spaces
        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(2,))
        self.action_space = spaces.Box(low=-0.1, high=0.1, shape=(2,))

        # sample
        self._sampled_bound = None
        pass

    @staticmethod
    def _init_params(params: List[RandomizationParameter]) -> dict:
        """
        Convert a list of parameters to dict.

        Args:
            params (List[RandomizationParameter]): A list of randomized parameters.

        Returns:
            dict
        """
        randomized = dict()
        for param in params:
            randomized[param.name] = param

        return randomized

    def randomizable_parameters(self) -> List[RandomizationParameter]:
        """
        Return a list of randomized parameters.

        :return: List[RandomizedParameter]
        """
        return self.RANDOMIZABLE_PARAMETERS

    def randomized_parameter(self, param_name: str) -> RandomizationParameter:
        """
        Return the randomized parameter.

        Args:
            param_name (str): Name of the parameter to return.

        Returns:
            RandomizationParameter
        """
        return self._randomized_parameters[param_name]

    def update_task(self, task: dict) -> None:
        """
        Sample a new goal position for the navigation task

        Returns:
            None
        """
        self._current_state = self._start_state
        self._elapsed_steps = 0

        # domain randomized
        self._goal_position = np.concatenate(
            [
                [task["x_position"]],
                [task["y_position"]],
            ],
            dtype=np.float32,
        )
        pass

    def reset(self) -> np.ndarray:
        """
        Resets the environment and returns the current observation.

        Returns:
            np.ndarray
        """
        self._current_state = self._start_state
        self._elapsed_steps = 0

        return self._current_state

    def step(self, action: np.ndarray) -> Tuple:
        """
        Take a step in the environment and return the corresponding observation, action, reward, plus additional
        info.

        Args:
            action (np.ndarray): ACtion to be taken in the environment.

        Returns:
            Tuple
        """
        if self._goal_position is None:
            raise ValueError("Call to `update_task` required before calling `step`.")

        self._elapsed_steps += 1
        action = np.clip(action, -0.1, 0.1)

        assert self.action_space.contains(action)
        self._current_state = self._current_state + action

        x_dist = self._current_state[0] - self._goal_position[0]
        y_dist = self._current_state[1] - self._goal_position[1]

        reward = -np.sqrt(x_dist**2 + y_dist**2)

        done = (np.abs(x_dist) < 0.01) and (np.abs(y_dist) < 0.01)
        time_exceeded = self.elapsed_steps >= self.max_episode_steps

        return self._current_state, reward, (done or time_exceeded), {}

    @property
    def observation_space(self) -> gym.Space:
        """
        Returns the observation space of the environment.

        Returns:
          gym.Space
        """
        return self._observation_space

    @observation_space.setter
    def observation_space(self, value: Any) -> None:
        """
        Set the observation space for the environment.

        Returns:
          gym.Space
        """
        self._observation_space = value

    @property
    def action_space(self) -> gym.Space:
        """
        Returns the action space

        Returns:
          gym.Space
        """
        return self._action_space

    @action_space.setter
    def action_space(self, value: Any) -> None:
        """
        Set the action space for the environment.

        Returns:
            gym.Space
        """
        self._action_space = value

    def get_spaces(self) -> Tuple[gym.Space, gym.Space]:
        """
        Get the observation space and the action space.

        Returns:
            Tuple
        """
        return self._observation_space, self._action_space

    @property
    def elapsed_steps(self) -> int:
        """
        Returns the elapsed number of episode steps in the environment.

        Returns:
          int
        """
        return self._elapsed_steps

    @property
    def max_episode_steps(self) -> int:
        """
        Returns the maximum number of episode steps in the environment.

        Returns:
          int
        """
        return self._max_episode_steps

    def render(self, mode: str = "human") -> None:
        """
        Render the environment given the render mode.

        Args:
          mode (str): Mode in which to render the environment.

        Returns:
          None
        """
        pass

    def close(self) -> None:
        """
        Close the environment.

        Returns:
          None
        """
        pass
